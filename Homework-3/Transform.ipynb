{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bjDhc_bsBq3-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.encoding[:, :x.size(1)].detach()\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, nhead):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.nhead = nhead\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        q = self.q_linear(q)\n",
        "        k = self.k_linear(k)\n",
        "        v = self.v_linear(v)\n",
        "\n",
        "        q = q.view(q.size(0), -1, self.nhead, self.d_model // self.nhead).transpose(1, 2)\n",
        "        k = k.view(k.size(0), -1, self.nhead, self.d_model // self.nhead).transpose(1, 2)\n",
        "        v = v.view(v.size(0), -1, self.nhead, self.d_model // self.nhead).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.d_model // self.nhead, dtype=torch.float32))\n",
        "\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attention = nn.functional.softmax(scores, dim=-1)\n",
        "        x = torch.matmul(attention, v)\n",
        "        x = x.transpose(1, 2).contiguous().view(x.size(0), -1, self.d_model)\n",
        "\n",
        "        return self.out(x)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, d_ff):\n",
        "        super(TransformerLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, nhead)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = x + self.dropout(attn_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "        x = self.norm2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_layers, d_ff):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model)\n",
        "        self.transformer_layers = nn.ModuleList([TransformerLayer(d_model, nhead, d_ff) for _ in range(num_layers)])\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.embedding(x)\n",
        "        x = x + self.positional_encoding(x)\n",
        "\n",
        "        for layer in self.transformer_layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Example usage:\n",
        "vocab_size = 10000  # Adjust based on your dataset vocabulary size\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "\n",
        "model = Transformer(vocab_size, d_model, nhead, num_layers, d_ff)\n",
        "\n",
        "# Define your input tensors (adjust shapes based on your dataset)\n",
        "src = torch.randint(0, vocab_size, (32, 32))  # (sequence length, batch size)\n",
        "mask = torch.ones_like(src)  # This is a simple example; you may need to create a proper mask based on your task\n",
        "\n",
        "output = model(src, mask)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bTDXPp-BtI7",
        "outputId": "62b20c3d-5e52-469f-b224-d0b2c315fb48"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.3185, -0.6270,  0.0854,  ...,  0.2607, -0.8709, -0.0693],\n",
              "         [-0.1849,  0.3812,  1.2874,  ...,  0.5064, -0.3868, -0.0705],\n",
              "         [ 0.5565, -0.1648,  0.4797,  ..., -0.0579, -0.5002, -0.2894],\n",
              "         ...,\n",
              "         [-0.3584, -0.2182, -0.4169,  ...,  0.4447,  0.1997, -0.1067],\n",
              "         [-0.1045, -0.4073, -0.4237,  ..., -0.1433, -0.7178,  0.1841],\n",
              "         [ 0.5390, -0.9265,  0.1892,  ...,  0.0154, -0.4392, -1.1138]],\n",
              "\n",
              "        [[-0.0867, -0.8074,  0.8041,  ...,  0.4412, -1.2305,  0.5416],\n",
              "         [ 1.1327,  0.0483, -0.9722,  ..., -0.5373, -0.2334,  0.8293],\n",
              "         [ 0.0269, -0.2483, -0.3554,  ..., -0.0077, -0.2522, -0.0726],\n",
              "         ...,\n",
              "         [-0.0325, -0.4224,  0.4529,  ..., -0.2559, -0.5969,  0.4381],\n",
              "         [ 0.0413, -1.1664,  0.6749,  ...,  0.4957, -0.7693, -0.1099],\n",
              "         [-0.5257, -0.5453,  0.8009,  ..., -0.8668, -1.2717,  0.8335]],\n",
              "\n",
              "        [[ 0.5855, -0.5652, -0.4818,  ...,  0.7361, -0.6954,  0.8023],\n",
              "         [-0.2199, -0.1594,  0.6121,  ...,  0.4401, -1.0646,  1.1238],\n",
              "         [ 1.2210,  0.0856,  0.3865,  ...,  0.0706, -1.0036,  0.9065],\n",
              "         ...,\n",
              "         [-0.5427, -0.8331,  0.5650,  ...,  0.2410,  0.6518,  0.6194],\n",
              "         [-0.2199, -0.7201,  0.5074,  ...,  0.2110, -1.0581, -0.0270],\n",
              "         [ 0.2866, -0.3610,  0.4160,  ...,  0.7047,  0.4614, -1.2815]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[-0.3358, -0.0190, -0.4068,  ...,  0.5182, -0.2032,  0.2279],\n",
              "         [-0.2349, -0.5643,  0.5550,  ..., -0.1158, -0.3906,  1.0994],\n",
              "         [-0.3740, -0.8419,  0.3938,  ..., -0.5092, -0.2095, -0.5897],\n",
              "         ...,\n",
              "         [ 0.0583, -0.0359,  0.5773,  ...,  0.7044, -0.4823,  0.6751],\n",
              "         [-0.1177, -0.3682,  0.7070,  ..., -0.0549, -0.2215,  0.0267],\n",
              "         [ 0.3861,  0.4928,  0.6284,  ...,  0.3042, -0.3139,  0.8969]],\n",
              "\n",
              "        [[-0.6008, -0.3080,  0.4794,  ...,  0.1484, -0.0139,  0.2188],\n",
              "         [-0.0543, -0.1486,  0.7822,  ...,  0.5735, -0.1805, -0.2985],\n",
              "         [-0.6610, -0.2888,  0.5042,  ..., -0.5393, -0.7753,  0.2989],\n",
              "         ...,\n",
              "         [ 0.6964, -0.9447,  0.6858,  ..., -0.3498, -1.2329, -0.2904],\n",
              "         [-0.4677, -0.4850,  0.1051,  ..., -0.3022, -0.5997,  0.3382],\n",
              "         [ 0.0924,  0.4590, -0.4122,  ..., -0.3944,  0.4047, -0.0813]],\n",
              "\n",
              "        [[-0.5493,  0.0645, -0.3491,  ...,  0.5780,  0.0437, -0.7736],\n",
              "         [-0.4171,  0.4674, -0.1897,  ...,  0.6833,  0.2534, -0.0449],\n",
              "         [ 0.1251, -0.8139,  0.4468,  ...,  0.0928, -0.4716,  0.6076],\n",
              "         ...,\n",
              "         [ 0.2599,  0.5788,  0.4192,  ..., -0.6418, -0.5243, -0.1213],\n",
              "         [ 0.0517, -0.7719,  0.1696,  ..., -0.1284, -0.0218,  0.5488],\n",
              "         [-0.1941, -0.5649,  1.1706,  ..., -0.0243, -0.8956,  1.0562]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Dummy dataset\n",
        "class DummyDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, vocab_size, seq_length, num_samples):\n",
        "        self.data = torch.randint(0, vocab_size, (num_samples, seq_length))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# Create a dummy dataset\n",
        "vocab_size = 1000\n",
        "seq_length = 32\n",
        "num_samples = 100\n",
        "dummy_dataset = DummyDataset(vocab_size, seq_length, num_samples)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_size = int(0.8 * len(dummy_dataset))\n",
        "test_size = len(dummy_dataset) - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(dummy_dataset, [train_size, test_size])\n",
        "\n",
        "# DataLoader for training and testing\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Instantiate the model, loss function, and optimizer\n",
        "vocab_size = 1000\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "model = Transformer(vocab_size, d_model, nhead, num_layers, d_ff)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for inputs in train_loader:\n",
        "        src = inputs[:, :-1]  # Input sequence\n",
        "        tgt = inputs[:, 1:]   # Target sequence (shifted by one position)\n",
        "        mask = (src != 0).unsqueeze(1).unsqueeze(2)     # Create a mask to ignore padding tokens\n",
        "\n",
        "        if mask is not None:\n",
        "          mask = mask.to(torch.bool)  # Ensure the mask is of boolean type\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(src, mask)\n",
        "        loss = criterion(outputs.reshape(-1, vocab_size), tgt.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5luQGE-LB866",
        "outputId": "86dd0906-bc5a-4da5-c0c2-58512ef39a3a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 7.1226372718811035\n",
            "Epoch 2/10, Loss: 6.9523091316223145\n",
            "Epoch 3/10, Loss: 6.47813081741333\n",
            "Epoch 4/10, Loss: 5.6671600341796875\n",
            "Epoch 5/10, Loss: 4.596777439117432\n",
            "Epoch 6/10, Loss: 4.534095287322998\n",
            "Epoch 7/10, Loss: 3.3199002742767334\n",
            "Epoch 8/10, Loss: 2.3391032218933105\n",
            "Epoch 9/10, Loss: 1.5963609218597412\n",
            "Epoch 10/10, Loss: 1.0564284324645996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing loop\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "with torch.no_grad():\n",
        "    for inputs in test_loader:\n",
        "        src = inputs[:, :-1]  # Input sequence\n",
        "        tgt = inputs[:, 1:]   # Target sequence (shifted by one position)\n",
        "        mask = (src != 0).unsqueeze(1).unsqueeze(2)     # Create a mask to ignore padding tokens\n",
        "\n",
        "        if mask is not None:\n",
        "          mask = mask.to(torch.bool)  # Ensure the mask is of boolean type\n",
        "\n",
        "        outputs = model(src, mask)\n",
        "        _, predictions = torch.max(outputs, dim=-1)\n",
        "\n",
        "        total_correct += (predictions == tgt).sum().item()\n",
        "        total_samples += tgt.numel()\n",
        "\n",
        "accuracy = total_correct / total_samples\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxqDLK-3C29v",
        "outputId": "5eadf0fb-bcc2-423b-eccb-47338e536c9b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.32%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cmQ6CFu0FHwd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}